# -*- coding: utf-8 -*-
"""translation_analysis_entropy.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14HVbkuVf_n_JjvpNHFY_qh5xqjWHO-fj
"""

# -*- coding: utf-8 -*-
"""
Enhanced Translation Entropy Analysis Module with Manual Methods - FIXED VERSION

This module provides four different analysis approaches with fixes for:
1. SpaCy alignment issues
2. Autoregressive statistics calculation
3. Surprisal calculation problems
4. NaN handling

Analysis methods:
1. FULL: Natural seq2seq autoregressive generation (uses seq2seq model)
2. CONDITIONAL: Conditional probability reconstruction (uses conditional model)
3. MANUAL_AUTOREGRESSIVE: Manual masking with autoregressive context (uses both models)
4. MANUAL_BIGRAM_DECODER: Manual masking with bigram decoder context (uses both models)
"""

import torch
import torch.nn.functional as F
import math
import json
import pandas as pd
import numpy as np
import os
from typing import List, Dict, Any, Tuple, Optional
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, MBartForConditionalGeneration, MBart50Tokenizer
import spacy
from entmax import entmax15
import gc

# Set CUDA debugging environment variables
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
os.environ['TORCH_USE_CUDA_DSA'] = '1'
os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'


class EnhancedTranslationEntropyAnalyzer:
    """Main class for four-way translation entropy analysis - FIXED VERSION."""

    def __init__(self, model_name: str = "facebook/mbart-large-50-many-to-many-mmt",
                 src_lang: str = "pl_PL", tgt_lang: str = "en_XX"):
        """Initialize the analyzer with model and language settings."""
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model_name = model_name
        self.src_lang = src_lang
        self.tgt_lang = tgt_lang

        # Initialize models and tokenizers
        self._load_models()
        self._load_spacy_models()

    def _load_models(self):
        """Load both seq2seq and conditional models for different analysis approaches."""
        print(f"Loading models on {self.device}...")

        # Seq2seq model setup (for FULL analysis)
        self.seq2seq_tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.seq2seq_model = AutoModelForSeq2SeqLM.from_pretrained(self.model_name).to(self.device)
        self.seq2seq_model.eval()

        # Conditional generation model setup (for CONDITIONAL and MANUAL analyses)
        self.conditional_tokenizer = MBart50Tokenizer.from_pretrained(self.model_name)
        self.conditional_model = MBartForConditionalGeneration.from_pretrained(self.model_name).to(self.device)
        self.conditional_model.eval()

        # Set language configurations
        self.seq2seq_tokenizer.src_lang = self.src_lang
        self.conditional_tokenizer.src_lang = self.src_lang
        self.forced_bos_token_id = self.seq2seq_tokenizer.convert_tokens_to_ids(self.tgt_lang)

        # Set mask token (use seq2seq tokenizer as primary)
        self.mask_token_id = (self.seq2seq_tokenizer.mask_token_id
                             if self.seq2seq_tokenizer.mask_token_id is not None
                             else self.seq2seq_tokenizer.unk_token_id)

        if self.seq2seq_tokenizer.mask_token_id is None:
            print("Warning: [MASK] token not available for mBART. Using <unk> as placeholder.")

        print(f"Loaded seq2seq model: {type(self.seq2seq_model).__name__}")
        print(f"Loaded conditional model: {type(self.conditional_model).__name__}")
        print(f"Seq2seq tokenizer type: {type(self.seq2seq_tokenizer).__name__}")
        print(f"Conditional tokenizer type: {type(self.conditional_tokenizer).__name__}")

    def _load_spacy_models(self):
        """Load spaCy models for linguistic annotation."""
        try:
            self.nlp_src = spacy.load("pl_core_news_sm")  # Polish
            self.nlp_tgt = spacy.load("en_core_web_sm")   # English
            print("SpaCy models loaded successfully")
        except OSError as e:
            print(f"Warning: Could not load spaCy models. Error: {e}")
            self.nlp_src = None
            self.nlp_tgt = None

    @staticmethod
    def entropy(probs: torch.Tensor) -> float:
        """Calculate entropy from probability distribution."""
        return -(probs * probs.log2()).sum().item()

    @staticmethod
    def entmax_entropy(probs: torch.Tensor, epsilon: float = 1e-10) -> float:
        """Calculate entropy for entmax15 probabilities."""
        log_probs = torch.log2(probs + epsilon)
        entropy = -(probs * log_probs).sum()
        return entropy.item()

    @staticmethod
    def topk_entropy(probs: torch.Tensor, k: int = 5) -> Tuple[float, float]:
        """Calculate top-k entropy and mass."""
        topk_probs, _ = torch.topk(probs, k)
        topk_sum = topk_probs.sum().item()
        if topk_sum > 0:
            normalized = topk_probs / topk_sum
            entropy = -(normalized * normalized.log2()).sum().item()
            return entropy, topk_sum
        else:
            return 0.0, 0.0

    @staticmethod
    def topk_entmax_entropy(probs: torch.Tensor, k: int = 5, epsilon: float = 1e-10) -> Tuple[float, float]:
        """Calculate top-k entropy for entmax15 probabilities."""
        topk_probs, _ = torch.topk(probs, k)
        topk_sum = topk_probs.sum().item()
        if topk_sum > 0:
            normalized = topk_probs / topk_sum
            log_probs = torch.log2(normalized + epsilon)
            entropy = -(normalized * log_probs).sum()
            return entropy.item(), topk_sum
        else:
            return 0.0, 0.0

    def shift_tokens_right(self, input_ids: torch.Tensor, pad_token_id: int) -> torch.Tensor:
        """Shift tokens right for decoder input."""
        shifted = input_ids.new_zeros(input_ids.shape)
        shifted[:, 1:] = input_ids[:, :-1].clone()
        shifted[:, 0] = pad_token_id
        return shifted

    def align_spacy_tags(self, tokens: List[str], spacy_doc, original_text: str = "") -> Dict[str, List[str]]:
        """
        FIXED: Align spaCy annotations with mBART tokenization.

        Key fixes:
        1. Properly handle ▁ markers for word boundaries
        2. Reconstruct words from subword tokens
        3. Align reconstructed words with spaCy tokens
        4. Handle character-level differences
        """
        if spacy_doc is None:
            return {
                "pos": ["UNK"] * len(tokens),
                "dep": ["UNK"] * len(tokens),
                "spacy_tokens": ["UNK"] * len(tokens)
            }

        tags = {"pos": [], "dep": [], "spacy_tokens": []}
        spacy_words = [token.text for token in spacy_doc]
        spacy_pos = [token.pos_ for token in spacy_doc]
        spacy_dep = [token.dep_ for token in spacy_doc]

        spacy_idx = 0
        current_word = ""
        word_start_idx = 0

        for i, tok in enumerate(tokens):
            # Handle special tokens
            if tok in ['<s>', '</s>', 'pl_PL', 'en_XX']:
                tags["pos"].append("SPECIAL")
                tags["dep"].append("SPECIAL")
                tags["spacy_tokens"].append(tok)
                continue

            # Check if this starts a new word (▁ prefix indicates word boundary in mBART)
            if tok.startswith('▁'):
                # If we have a current word, try to match it with spaCy
                if current_word and spacy_idx < len(spacy_words):
                    # Check if current reconstructed word matches spaCy word
                    if current_word.lower().strip() == spacy_words[spacy_idx].lower().strip():
                        spacy_idx += 1

                # Start new word
                current_word = tok[1:]  # Remove ▁ marker
                word_start_idx = i
            else:
                # Continuation of current word
                current_word += tok

            # Assign current spaCy tags to this token
            if spacy_idx < len(spacy_words):
                tags["pos"].append(spacy_pos[spacy_idx])
                tags["dep"].append(spacy_dep[spacy_idx])
                tags["spacy_tokens"].append(spacy_words[spacy_idx])
            else:
                tags["pos"].append("UNK")
                tags["dep"].append("UNK")
                tags["spacy_tokens"].append("UNK")

        return tags

    # ==================== ANALYSIS METHOD DESCRIPTIONS ====================

    def get_analysis_descriptions(self) -> Dict[str, str]:
        """
        Return descriptions of all analysis methods implemented.

        Returns:
            Dictionary mapping method names to their descriptions
        """
        return {
            "full": """
            FULL Analysis: Natural seq2seq autoregressive generation
            - Uses seq2seq_model (AutoModelForSeq2SeqLM) for autoregressive source analysis and target generation
            - Uses conditional_model (MBartForConditionalGeneration) for bigram analysis
            - Formula: P(token | autoregressive_context) via generation scores
            - Source: Autoregressive generation + bigram decoder context
            - Target: Translation via beam search generation + bigram conditional analysis
            - Models: seq2seq_model (primary) + conditional_model (bigrams)
            """,

            "conditional": """
            CONDITIONAL Analysis: Conditional probability reconstruction
            - Uses conditional_model (MBartForConditionalGeneration) exclusively
            - Formula: P(token | prefix_context) via conditional forward passes
            - Source: Step-by-step conditional reconstruction + bigram decoder context
            - Target: Conditional reconstruction with source context + bigram analysis
            - Model: conditional_model only
            """,

            "manual_autoregressive": """
            MANUAL_AUTOREGRESSIVE Analysis: Manual masking with autoregressive context
            - Uses both seq2seq_model and conditional_model for comprehensive analysis
            - Formula: P(token | all_previous_tokens) via masking at each position
            - Source: Autoregressive context with manual masking (both models) + bigram decoder masking
            - Target: Same approach applied to translation candidates
            - Models: Both seq2seq_model and conditional_model
            - Method: Places MASK token at target position, uses all previous tokens as context
            - Keys: manual_autoregressive_seq2seq_*, manual_autoregressive_conditional_*, manual_bigram_decoder_*
            """,

            "manual_bigram_decoder": """
            MANUAL_BIGRAM_DECODER Analysis: Manual masking with bigram decoder context
            - Uses conditional_model (MBartForConditionalGeneration) for bigram context
            - Formula: P(token | previous_token) via decoder masking
            - Source: Bigram context with decoder masking
            - Target: Same approach applied to translation candidates
            - Model: conditional_model only
            - Method: Uses only immediate previous token as context, masks current position
            - Keys: manual_bigram_decoder_*
            """
        }

    # ==================== FULL ANALYSIS (Seq2Seq Generation) ====================

    def analyze_full(self, sentences: List[str], num_beams: int = 10,
                    num_return_sequences: int = 10) -> Dict[str, Any]:
        """
        FULL Analysis: Natural seq2seq autoregressive generation.
        Formula: P(token | autoregressive_context) via generation scores.
        Uses seq2seq_model (AutoModelForSeq2SeqLM).
        """
        print("=== FULL ANALYSIS (Seq2Seq Generation) ===")
        results = {}

        for sentence in sentences:
            print(f"Processing FULL analysis for: {sentence[:50]}...")

            # Set source language
            self.seq2seq_tokenizer.src_lang = self.src_lang

            # Tokenize input
            inputs = self.seq2seq_tokenizer(sentence, return_tensors="pt").to(self.device)
            input_ids = inputs["input_ids"][0]
            source_token_strs = self.seq2seq_tokenizer.convert_ids_to_tokens(input_ids)

            # Source analysis via autoregressive generation
            source_metrics = self._analyze_full_source(sentence, source_token_strs)

            # Bigram source analysis
            bigram_source_metrics = self._analyze_full_bigram_source(sentence, source_token_strs)

            # Get spaCy annotations
            doc_src = self.nlp_src(sentence) if self.nlp_src else None
            aligned_src = self.align_spacy_tags(source_token_strs, doc_src, sentence)

            source_info = {
                "tokens": source_token_strs,
                **source_metrics,
                **bigram_source_metrics,
                "pos": aligned_src["pos"],
                "dep": aligned_src["dep"],
                "spacy_tokens": aligned_src["spacy_tokens"],
                "analysis_type": "full"
            }

            # Target analysis via translation generation
            candidates = self._analyze_full_targets(sentence, num_beams, num_return_sequences)

            results[sentence] = {
                'source_info': source_info,
                'candidates': candidates
            }

        return results

    def _analyze_full_source(self, sentence: str, source_token_strs: List[str]) -> Dict[str, Dict[str, float]]:
        """Analyze source tokens via autoregressive generation using seq2seq model."""
        self.seq2seq_tokenizer.src_lang = self.src_lang
        inputs = self.seq2seq_tokenizer(sentence, return_tensors="pt").to(self.device)

        with torch.no_grad():
            outputs = self.seq2seq_model.generate(
                inputs.input_ids,
                max_new_tokens=len(source_token_strs),
                output_scores=True,
                return_dict_in_generate=True,
                do_sample=False,
                forced_bos_token_id=self.seq2seq_tokenizer.convert_tokens_to_ids(self.src_lang)
            )

            if hasattr(outputs, 'scores') and outputs.scores:
                scores = outputs.scores
                entropies, surprisals, topk_entropies, topk_masses = [], [], [], []
                entmax_entropies, entmax_surprisals, entmax_topk_entropies, entmax_topk_masses, entmax_branchings = [], [], [], [], []

                for i, logits in enumerate(scores):
                    probs = F.softmax(logits[0], dim=-1)
                    entmax_probs = entmax15(logits[0].cpu(), dim=-1).to(self.device)

                    # Standard softmax metrics
                    entropy = self.entropy(probs)
                    topk_ent, topk_sum = self.topk_entropy(probs, k=5)
                    entropies.append(entropy)
                    topk_entropies.append(topk_ent)
                    topk_masses.append(topk_sum)

                    # Entmax15 metrics
                    entmax_entropy = self.entmax_entropy(entmax_probs)
                    entmax_topk_ent, entmax_topk_sum = self.topk_entmax_entropy(entmax_probs, k=5)
                    entmax_branching = torch.count_nonzero(entmax_probs).item()
                    entmax_entropies.append(entmax_entropy)
                    entmax_topk_entropies.append(entmax_topk_ent)
                    entmax_topk_masses.append(entmax_topk_sum)
                    entmax_branchings.append(entmax_branching)

                    # Get surprisal for the actual token
                    if i < len(inputs.input_ids[0]):
                        token_id = inputs.input_ids[0][i].item()
                        surprisal = -math.log2(probs[token_id].item()) if probs[token_id] > 0 else 20.0
                        entmax_surprisal = -math.log2(entmax_probs[token_id].item()) if entmax_probs[token_id] > 0 else 20.0
                        surprisals.append(surprisal)
                        entmax_surprisals.append(entmax_surprisal)
                    else:
                        surprisals.append(0.0)
                        entmax_surprisals.append(0.0)
            else:
                # Fallback: use uniform values
                n_tokens = len(source_token_strs)
                entropies = surprisals = topk_entropies = topk_masses = [0.0] * n_tokens
                entmax_entropies = entmax_surprisals = entmax_topk_entropies = entmax_topk_masses = [0.0] * n_tokens
                entmax_branchings = [0] * n_tokens

        return {
            "full_source_entropy_bits": dict(zip(source_token_strs, entropies)),
            "full_source_surprisal_bits": dict(zip(source_token_strs, surprisals)),
            "full_source_topk_entropy_k5_bits": dict(zip(source_token_strs, topk_entropies)),
            "full_source_topk_mass_k5": dict(zip(source_token_strs, topk_masses)),
            "full_source_entmax_entropy_bits": dict(zip(source_token_strs, entmax_entropies)),
            "full_source_entmax_surprisal_bits": dict(zip(source_token_strs, entmax_surprisals)),
            "full_source_entmax_topk_entropy_k5_bits": dict(zip(source_token_strs, entmax_topk_entropies)),
            "full_source_entmax_topk_mass_k5": dict(zip(source_token_strs, entmax_topk_masses)),
            "full_source_entmax_branching_choices": dict(zip(source_token_strs, entmax_branchings))
        }

    def _analyze_full_bigram_source(self, sentence: str, source_token_strs: List[str]) -> Dict[str, Dict[str, float]]:
        """Analyze source bigrams via decoder conditioning using conditional model (for FULL analysis bigrams)."""
        # Convert tokens from seq2seq tokenizer to conditional tokenizer format
        # Use conditional model for bigram analysis even in FULL approach
        self.conditional_tokenizer.src_lang = self.src_lang
        tokenized = self.conditional_tokenizer(sentence, return_tensors="pt").to(self.device)
        input_ids = tokenized.input_ids
        conditional_tokens = self.conditional_tokenizer.convert_ids_to_tokens(input_ids[0])

        decoder_input_ids = self.shift_tokens_right(input_ids, self.conditional_tokenizer.pad_token_id)

        entropies, surprisals, topk_entropies, topk_masses = [], [], [], []
        entmax_entropies, entmax_surprisals, entmax_topk_entropies, entmax_topk_masses, entmax_branchings = [], [], [], [], []

        with torch.no_grad():
            outputs = self.conditional_model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)
            logits = outputs.logits
            probs = F.softmax(logits, dim=-1)

            for i in range(len(conditional_tokens)):
                if i < logits.size(1):
                    prob_dist = probs[0, i]
                    entmax_prob_dist = entmax15(logits[0, i].cpu(), dim=-1).to(self.device)

                    # Standard softmax metrics
                    entropy = self.entropy(prob_dist)
                    topk_ent, topk_sum = self.topk_entropy(prob_dist, k=5)
                    entropies.append(entropy)
                    topk_entropies.append(topk_ent)
                    topk_masses.append(topk_sum)

                    # Entmax15 metrics
                    entmax_entropy = self.entmax_entropy(entmax_prob_dist)
                    entmax_topk_ent, entmax_topk_sum = self.topk_entmax_entropy(entmax_prob_dist, k=5)
                    entmax_branching = torch.count_nonzero(entmax_prob_dist).item()
                    entmax_entropies.append(entmax_entropy)
                    entmax_topk_entropies.append(entmax_topk_ent)
                    entmax_topk_masses.append(entmax_topk_sum)
                    entmax_branchings.append(entmax_branching)

                    if i < input_ids.size(1):
                        token_id = input_ids[0, i].item()
                        surprisal = -math.log2(prob_dist[token_id].item()) if prob_dist[token_id] > 0 else 20.0
                        entmax_surprisal = -math.log2(entmax_prob_dist[token_id].item()) if entmax_prob_dist[token_id] > 0 else 20.0
                        surprisals.append(surprisal)
                        entmax_surprisals.append(entmax_surprisal)
                    else:
                        surprisals.append(0.0)
                        entmax_surprisals.append(0.0)
                else:
                    entropies.append(0.0)
                    surprisals.append(0.0)
                    topk_entropies.append(0.0)
                    topk_masses.append(0.0)
                    entmax_entropies.append(0.0)
                    entmax_surprisals.append(0.0)
                    entmax_topk_entropies.append(0.0)
                    entmax_topk_masses.append(0.0)
                    entmax_branchings.append(0)

        # Ensure we match the length of source_token_strs
        while len(entropies) < len(source_token_strs):
            entropies.append(0.0)
            surprisals.append(0.0)
            topk_entropies.append(0.0)
            topk_masses.append(0.0)
            entmax_entropies.append(0.0)
            entmax_surprisals.append(0.0)
            entmax_topk_entropies.append(0.0)
            entmax_topk_masses.append(0.0)
            entmax_branchings.append(0)

        return {
            "full_bigram_source_entropy_bits": dict(zip(source_token_strs, entropies)),
            "full_bigram_source_surprisal_bits": dict(zip(source_token_strs, surprisals)),
            "full_bigram_source_topk_entropy_k5_bits": dict(zip(source_token_strs, topk_entropies)),
            "full_bigram_source_topk_mass_k5": dict(zip(source_token_strs, topk_masses)),
            "full_bigram_source_entmax_entropy_bits": dict(zip(source_token_strs, entmax_entropies)),
            "full_bigram_source_entmax_surprisal_bits": dict(zip(source_token_strs, entmax_surprisals)),
            "full_bigram_source_entmax_topk_entropy_k5_bits": dict(zip(source_token_strs, entmax_topk_entropies)),
            "full_bigram_source_entmax_topk_mass_k5": dict(zip(source_token_strs, entmax_topk_masses)),
            "full_bigram_source_entmax_branching_choices": dict(zip(source_token_strs, entmax_branchings))
        }

    def _analyze_full_targets(self, sentence: str, num_beams: int, num_return_sequences: int) -> List[Dict[str, Any]]:
        """Analyze targets via autoregressive translation generation using seq2seq model."""
        self.seq2seq_tokenizer.src_lang = self.src_lang
        inputs = self.seq2seq_tokenizer(sentence, return_tensors="pt").to(self.device)

        with torch.no_grad():
            outputs = self.seq2seq_model.generate(
                inputs.input_ids,
                max_new_tokens=50,
                num_beams=num_beams,
                num_return_sequences=num_return_sequences,
                output_scores=True,
                return_dict_in_generate=True,
                forced_bos_token_id=self.seq2seq_tokenizer.convert_tokens_to_ids(self.tgt_lang),
                early_stopping=True
            )

        candidates = []
        sequences = outputs.sequences
        scores = outputs.scores if hasattr(outputs, 'scores') else None

        for seq_idx, sequence in enumerate(sequences):
            candidate = self._process_full_target_candidate(sequence, scores, sentence, seq_idx)
            candidates.append(candidate)

        return candidates

    def _process_full_target_candidate(self, sequence: torch.Tensor, scores: List[torch.Tensor],
                                     source_sentence: str, seq_idx: int) -> Dict[str, Any]:
        """Process a single target candidate from autoregressive generation."""
        decoded = self.seq2seq_tokenizer.decode(sequence, skip_special_tokens=True)
        target_tokens = self.seq2seq_tokenizer.convert_ids_to_tokens(sequence)

        doc_tgt = self.nlp_tgt(decoded) if self.nlp_tgt else None
        aligned_tgt = self.align_spacy_tags(target_tokens, doc_tgt, decoded)

        # Calculate target metrics from generation scores
        entropies, surprisals, topk_entropies, topk_masses = [], [], [], []
        entmax_entropies, entmax_surprisals, entmax_topk_entropies, entmax_topk_masses, entmax_branchings = [], [], [], [], []

        if scores:
            for i, logits in enumerate(scores):
                if seq_idx < logits.size(0):
                    probs = F.softmax(logits[seq_idx], dim=-1)
                    entmax_probs = entmax15(logits[seq_idx].cpu(), dim=-1).to(self.device)

                    # Standard softmax metrics
                    entropy = self.entropy(probs)
                    topk_ent, topk_sum = self.topk_entropy(probs, k=5)
                    entropies.append(entropy)
                    topk_entropies.append(topk_ent)
                    topk_masses.append(topk_sum)

                    # Entmax15 metrics
                    entmax_entropy = self.entmax_entropy(entmax_probs)
                    entmax_topk_ent, entmax_topk_sum = self.topk_entmax_entropy(entmax_probs, k=5)
                    entmax_branching = torch.count_nonzero(entmax_probs).item()
                    entmax_entropies.append(entmax_entropy)
                    entmax_topk_entropies.append(entmax_topk_ent)
                    entmax_topk_masses.append(entmax_topk_sum)
                    entmax_branchings.append(entmax_branching)

                    # Get surprisal for generated token
                    if i + 1 < len(sequence):
                        token_id = sequence[i + 1].item()
                        surprisal = -math.log2(probs[token_id].item()) if probs[token_id] > 0 else 20.0
                        entmax_surprisal = -math.log2(entmax_probs[token_id].item()) if entmax_probs[token_id] > 0 else 20.0
                        surprisals.append(surprisal)
                        entmax_surprisals.append(entmax_surprisal)
                    else:
                        surprisals.append(0.0)
                        entmax_surprisals.append(0.0)
                else:
                    entropies.append(0.0)
                    surprisals.append(0.0)
                    topk_entropies.append(0.0)
                    topk_masses.append(0.0)
                    entmax_entropies.append(0.0)
                    entmax_surprisals.append(0.0)
                    entmax_topk_entropies.append(0.0)
                    entmax_topk_masses.append(0.0)
                    entmax_branchings.append(0)

        # Pad to match token count
        while len(entropies) < len(target_tokens):
            for lst in [entropies, surprisals, topk_entropies, topk_masses,
                       entmax_entropies, entmax_surprisals, entmax_topk_entropies, entmax_topk_masses]:
                lst.append(0.0)
            entmax_branchings.append(0)

        # Calculate bigram metrics
        bigram_metrics = self._calculate_full_target_bigrams(source_sentence, decoded)

        return {
            "sentence": decoded,
            "tokens": target_tokens,
            "full_target_entropy_bits": dict(zip(target_tokens, entropies)),
            "full_target_surprisal_bits": dict(zip(target_tokens, surprisals)),
            "full_target_topk_entropy_k5_bits": dict(zip(target_tokens, topk_entropies)),
            "full_target_topk_mass_k5": dict(zip(target_tokens, topk_masses)),
            "full_target_entmax_entropy_bits": dict(zip(target_tokens, entmax_entropies)),
            "full_target_entmax_surprisal_bits": dict(zip(target_tokens, entmax_surprisals)),
            "full_target_entmax_topk_entropy_k5_bits": dict(zip(target_tokens, entmax_topk_entropies)),
            "full_target_entmax_topk_mass_k5": dict(zip(target_tokens, entmax_topk_masses)),
            "full_target_entmax_branching_choices": dict(zip(target_tokens, entmax_branchings)),
            **bigram_metrics,
            "pos": aligned_tgt["pos"],
            "dep": aligned_tgt["dep"],
            "spacy_tokens": aligned_tgt["spacy_tokens"],
            "candidate_idx": seq_idx
        }

    def _calculate_full_target_bigrams(self, source_sentence: str, target_sentence: str) -> Dict[str, Dict[str, float]]:
        """Calculate bigram metrics for target using full conditional approach."""
        self.conditional_tokenizer.src_lang = self.src_lang
        source_inputs = self.conditional_tokenizer(source_sentence, return_tensors="pt").to(self.device)

        self.conditional_tokenizer.src_lang = self.tgt_lang
        target_inputs = self.conditional_tokenizer(target_sentence, return_tensors="pt").to(self.device)
        target_ids = target_inputs.input_ids
        target_tokens = self.conditional_tokenizer.convert_ids_to_tokens(target_ids[0])

        decoder_input_ids = self.shift_tokens_right(target_ids, self.conditional_tokenizer.pad_token_id)

        entropies, surprisals, topk_entropies, topk_masses = [], [], [], []
        entmax_entropies, entmax_surprisals, entmax_topk_entropies, entmax_topk_masses, entmax_branchings = [], [], [], [], []

        with torch.no_grad():
            outputs = self.conditional_model(input_ids=source_inputs.input_ids, decoder_input_ids=decoder_input_ids)
            logits = outputs.logits
            probs = F.softmax(logits, dim=-1)

            for i in range(target_ids.size(1)):
                if i < probs.size(1):
                    prob_dist = probs[0, i]
                    entmax_prob_dist = entmax15(logits[0, i].cpu(), dim=-1).to(self.device)

                    # Standard softmax metrics
                    entropy = self.entropy(prob_dist)
                    topk_ent, topk_sum = self.topk_entropy(prob_dist, k=5)
                    entropies.append(entropy)
                    topk_entropies.append(topk_ent)
                    topk_masses.append(topk_sum)

                    # Entmax15 metrics
                    entmax_entropy = self.entmax_entropy(entmax_prob_dist)
                    entmax_topk_ent, entmax_topk_sum = self.topk_entmax_entropy(entmax_prob_dist, k=5)
                    entmax_branching = torch.count_nonzero(entmax_prob_dist).item()
                    entmax_entropies.append(entmax_entropy)
                    entmax_topk_entropies.append(entmax_topk_ent)
                    entmax_topk_masses.append(entmax_topk_sum)
                    entmax_branchings.append(entmax_branching)

                    if i < target_ids.size(1):
                        token_id = target_ids[0, i].item()
                        surprisal = -math.log2(prob_dist[token_id].item()) if prob_dist[token_id] > 0 else 20.0
                        entmax_surprisal = -math.log2(entmax_prob_dist[token_id].item()) if entmax_prob_dist[token_id] > 0 else 20.0
                        surprisals.append(surprisal)
                        entmax_surprisals.append(entmax_surprisal)
                    else:
                        surprisals.append(0.0)
                        entmax_surprisals.append(0.0)
                else:
                    for lst in [entropies, surprisals, topk_entropies, topk_masses,
                               entmax_entropies, entmax_surprisals, entmax_topk_entropies, entmax_topk_masses]:
                        lst.append(0.0)
                    entmax_branchings.append(0)

        return {
            "full_bigram_target_entropy_bits": dict(zip(target_tokens, entropies)),
            "full_bigram_target_surprisal_bits": dict(zip(target_tokens, surprisals)),
            "full_bigram_target_topk_entropy_k5_bits": dict(zip(target_tokens, topk_entropies)),
            "full_bigram_target_topk_mass_k5": dict(zip(target_tokens, topk_masses)),
            "full_bigram_target_entmax_entropy_bits": dict(zip(target_tokens, entmax_entropies)),
            "full_bigram_target_entmax_surprisal_bits": dict(zip(target_tokens, entmax_surprisals)),
            "full_bigram_target_entmax_topk_entropy_k5_bits": dict(zip(target_tokens, entmax_topk_entropies)),
            "full_bigram_target_entmax_topk_mass_k5": dict(zip(target_tokens, entmax_topk_masses)),
            "full_bigram_target_entmax_branching_choices": dict(zip(target_tokens, entmax_branchings))
        }

    # ==================== CONDITIONAL ANALYSIS ====================

    def analyze_conditional(self, sentences: List[str], num_beams: int = 10,
                          num_return_sequences: int = 10) -> Dict[str, Any]:
        """
        CONDITIONAL Analysis: Conditional probability reconstruction.
        Formula: P(token | prefix_context) via conditional forward passes.
        Uses conditional_model (MBartForConditionalGeneration).
        """
        print("=== CONDITIONAL ANALYSIS ===")
        results = {}

        for sentence in sentences:
            print(f"Processing CONDITIONAL analysis for: {sentence[:50]}...")

            # Source analysis via conditional reconstruction
            source_metrics, source_tokens = self._analyze_conditional_source(sentence)
            bigram_source_metrics = self._analyze_conditional_bigram_source(sentence, source_tokens)

            # Get spaCy annotations
            doc_src = self.nlp_src(sentence) if self.nlp_src else None
            aligned_src = self.align_spacy_tags(source_tokens, doc_src, sentence)

            source_info = {
                "tokens": source_tokens,
                **source_metrics,
                **bigram_source_metrics,
                "pos": aligned_src["pos"],
                "dep": aligned_src["dep"],
                "spacy_tokens": aligned_src["spacy_tokens"],
                "analysis_type": "conditional"
            }

            # Target analysis via conditional reconstruction
            candidates = self._analyze_conditional_targets(sentence, num_beams, num_return_sequences)

            results[sentence] = {
                'source_info': source_info,
                'candidates': candidates
            }

        return results

    def _analyze_conditional_source(self, sentence: str) -> Tuple[Dict[str, Dict[str, float]], List[str]]:
        """Analyze source via conditional reconstruction using conditional model."""
        self.conditional_tokenizer.src_lang = self.src_lang
        inputs = self.conditional_tokenizer(sentence, return_tensors="pt").to(self.device)
        input_ids = inputs.input_ids[0]
        source_tokens = self.conditional_tokenizer.convert_ids_to_tokens(input_ids)

        entropies, surprisals, topk_entropies, topk_masses = [], [], [], []
        entmax_entropies, entmax_surprisals, entmax_topk_entropies, entmax_topk_masses, entmax_branchings = [], [], [], [], []

        with torch.no_grad():
            for i in range(1, len(input_ids)):
                # Context: all tokens up to position i-1
                context_ids = input_ids[:i]
                curr_token_id = input_ids[i].item()

                # Create decoder input
                decoder_input_ids = self.shift_tokens_right(context_ids.unsqueeze(0), self.conditional_tokenizer.pad_token_id)

                # Get conditional probability
                outputs = self.conditional_model(input_ids=context_ids.unsqueeze(0), decoder_input_ids=decoder_input_ids)
                logits = outputs.logits[0, -1]
                probs = F.softmax(logits, dim=-1)
                entmax_probs = entmax15(logits.cpu(), dim=-1).to(self.device)

                # Standard softmax metrics
                entropy = self.entropy(probs)
                topk_ent, topk_sum = self.topk_entropy(probs, k=5)
                token_prob = probs[curr_token_id].item()
                surprisal = -math.log2(token_prob) if token_prob > 0 else 20.0

                entropies.append(entropy)
                surprisals.append(surprisal)
                topk_entropies.append(topk_ent)
                topk_masses.append(topk_sum)

                # Entmax15 metrics
                entmax_entropy = self.entmax_entropy(entmax_probs)
                entmax_topk_ent, entmax_topk_sum = self.topk_entmax_entropy(entmax_probs, k=5)
                entmax_token_prob = entmax_probs[curr_token_id].item()
                entmax_surprisal = -math.log2(entmax_token_prob) if entmax_token_prob > 0 else 20.0
                entmax_branching = torch.count_nonzero(entmax_probs).item()

                entmax_entropies.append(entmax_entropy)
                entmax_surprisals.append(entmax_surprisal)
                entmax_topk_entropies.append(entmax_topk_ent)
                entmax_topk_masses.append(entmax_topk_sum)
                entmax_branchings.append(entmax_branching)

        # Pad for first token
        for lst in [entropies, surprisals, topk_entropies, topk_masses,
                   entmax_entropies, entmax_surprisals, entmax_topk_entropies, entmax_topk_masses]:
            lst.insert(0, 0.0)
        entmax_branchings.insert(0, 0)

        metrics = {
            "conditional_source_entropy_bits": dict(zip(source_tokens, entropies)),
            "conditional_source_surprisal_bits": dict(zip(source_tokens, surprisals)),
            "conditional_source_topk_entropy_k5_bits": dict(zip(source_tokens, topk_entropies)),
            "conditional_source_topk_mass_k5": dict(zip(source_tokens, topk_masses)),
            "conditional_source_entmax_entropy_bits": dict(zip(source_tokens, entmax_entropies)),
            "conditional_source_entmax_surprisal_bits": dict(zip(source_tokens, entmax_surprisals)),
            "conditional_source_entmax_topk_entropy_k5_bits": dict(zip(source_tokens, entmax_topk_entropies)),
            "conditional_source_entmax_topk_mass_k5": dict(zip(source_tokens, entmax_topk_masses)),
            "conditional_source_entmax_branching_choices": dict(zip(source_tokens, entmax_branchings))
        }

        return metrics, source_tokens

    def _analyze_conditional_bigram_source(self, sentence: str, source_tokens: List[str]) -> Dict[str, Dict[str, float]]:
        """Analyze source bigrams via conditional decoder using conditional model."""
        self.conditional_tokenizer.src_lang = self.src_lang
        tokenized = self.conditional_tokenizer(sentence, return_tensors="pt").to(self.device)
        input_ids = tokenized.input_ids

        decoder_input_ids = self.shift_tokens_right(input_ids, self.conditional_tokenizer.pad_token_id)

        entropies, surprisals, topk_entropies, topk_masses = [], [], [], []
        entmax_entropies, entmax_surprisals, entmax_topk_entropies, entmax_topk_masses, entmax_branchings = [], [], [], [], []

        with torch.no_grad():
            outputs = self.conditional_model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)
            logits = outputs.logits
            probs = F.softmax(logits, dim=-1)

            for i in range(1, input_ids.size(1)):
                if i < probs.size(1):
                    prob_dist = probs[0, i]
                    entmax_prob_dist = entmax15(logits[0, i].cpu(), dim=-1).to(self.device)

                    # Standard softmax metrics
                    entropy = self.entropy(prob_dist)
                    topk_ent, topk_sum = self.topk_entropy(prob_dist, k=5)
                    curr_token_id = input_ids[0, i].item()
                    surprisal = -math.log2(prob_dist[curr_token_id].item()) if prob_dist[curr_token_id] > 0 else 20.0

                    entropies.append(entropy)
                    surprisals.append(surprisal)
                    topk_entropies.append(topk_ent)
                    topk_masses.append(topk_sum)

                    # Entmax15 metrics
                    entmax_entropy = self.entmax_entropy(entmax_prob_dist)
                    entmax_topk_ent, entmax_topk_sum = self.topk_entmax_entropy(entmax_prob_dist, k=5)
                    entmax_surprisal = -math.log2(entmax_prob_dist[curr_token_id].item()) if entmax_prob_dist[curr_token_id] > 0 else 20.0
                    entmax_branching = torch.count_nonzero(entmax_prob_dist).item()

                    entmax_entropies.append(entmax_entropy)
                    entmax_surprisals.append(entmax_surprisal)
                    entmax_topk_entropies.append(entmax_topk_ent)
                    entmax_topk_masses.append(entmax_topk_sum)
                    entmax_branchings.append(entmax_branching)
                else:
                    for lst in [entropies, surprisals, topk_entropies, topk_masses,
                               entmax_entropies, entmax_surprisals, entmax_topk_entropies, entmax_topk_masses]:
                        lst.append(0.0)
                    entmax_branchings.append(0)

        # Pad for first token
        for lst in [entropies, surprisals, topk_entropies, topk_masses,
                   entmax_entropies, entmax_surprisals, entmax_topk_entropies, entmax_topk_masses]:
            lst.insert(0, 0.0)
        entmax_branchings.insert(0, 0)

        return {
            "conditional_bigram_source_entropy_bits": dict(zip(source_tokens, entropies)),
            "conditional_bigram_source_surprisal_bits": dict(zip(source_tokens, surprisals)),
            "conditional_bigram_source_topk_entropy_k5_bits": dict(zip(source_tokens, topk_entropies)),
            "conditional_bigram_source_topk_mass_k5": dict(zip(source_tokens, topk_masses)),
            "conditional_bigram_source_entmax_entropy_bits": dict(zip(source_tokens, entmax_entropies)),
            "conditional_bigram_source_entmax_surprisal_bits": dict(zip(source_tokens, entmax_surprisals)),
            "conditional_bigram_source_entmax_topk_entropy_k5_bits": dict(zip(source_tokens, entmax_topk_entropies)),
            "conditional_bigram_source_entmax_topk_mass_k5": dict(zip(source_tokens, entmax_topk_masses)),
            "conditional_bigram_source_entmax_branching_choices": dict(zip(source_tokens, entmax_branchings))
        }

    def _analyze_conditional_targets(self, sentence: str, num_beams: int, num_return_sequences: int) -> List[Dict[str, Any]]:
        """Analyze targets via conditional reconstruction with beam search using conditional model."""
        self.conditional_tokenizer.src_lang = self.src_lang
        source_inputs = self.conditional_tokenizer(sentence, return_tensors="pt").to(self.device)

        with torch.no_grad():
            beam_outputs = self.conditional_model.generate(
                source_inputs.input_ids,
                max_new_tokens=50,
                num_beams=num_beams,
                num_return_sequences=num_return_sequences,
                forced_bos_token_id=self.conditional_tokenizer.convert_tokens_to_ids(self.tgt_lang),
                early_stopping=True,
                return_dict_in_generate=True,
                output_scores=True
            )

        candidates = []
        sequences = beam_outputs.sequences

        for seq_idx, sequence in enumerate(sequences):
            candidate = self._process_conditional_target_candidate(sequence, sentence, seq_idx)
            candidates.append(candidate)

        return candidates

    def _process_conditional_target_candidate(self, sequence: torch.Tensor, source_sentence: str, seq_idx: int) -> Dict[str, Any]:
        """Process target candidate via conditional reconstruction."""
        # Extract target part
        en_xx_token_id = self.conditional_tokenizer.convert_tokens_to_ids(self.tgt_lang)
        try:
            en_start_idx = (sequence == en_xx_token_id).nonzero(as_tuple=True)[0][0].item() + 1
        except IndexError:
            en_start_idx = 0

        target_ids = sequence[en_start_idx:]

        # Remove EOS if present
        eos_token_id = self.conditional_tokenizer.eos_token_id
        if len(target_ids) > 0 and target_ids[-1] == eos_token_id:
            target_ids = target_ids[:-1]

        decoded = self.conditional_tokenizer.decode(target_ids, skip_special_tokens=True)
        target_tokens = self.conditional_tokenizer.convert_ids_to_tokens(target_ids)

        # Get spaCy annotations
        doc_tgt = self.nlp_tgt(decoded) if self.nlp_tgt else None
        aligned_tgt = self.align_spacy_tags(target_tokens, doc_tgt, decoded)

        # Calculate conditional target metrics
        target_metrics = self._calculate_conditional_target_metrics(source_sentence, target_ids)
        bigram_metrics = self._calculate_conditional_target_bigrams(source_sentence, decoded)

        return {
            "sentence": decoded,
            "tokens": target_tokens,
            **target_metrics,
            **bigram_metrics,
            "pos": aligned_tgt["pos"],
            "dep": aligned_tgt["dep"],
            "spacy_tokens": aligned_tgt["spacy_tokens"],
            "candidate_idx": seq_idx
        }

    def _calculate_conditional_target_metrics(self, source_sentence: str, target_ids: torch.Tensor) -> Dict[str, Dict[str, float]]:
        """Calculate conditional target metrics via reconstruction using conditional model."""
        self.conditional_tokenizer.src_lang = self.src_lang
        source_inputs = self.conditional_tokenizer(source_sentence, return_tensors="pt").to(self.device)
        source_ids = source_inputs.input_ids

        target_tokens = self.conditional_tokenizer.convert_ids_to_tokens(target_ids)
        entropies, surprisals, topk_entropies, topk_masses = [], [], [], []
        entmax_entropies, entmax_surprisals, entmax_topk_entropies, entmax_topk_masses, entmax_branchings = [], [], [], [], []

        with torch.no_grad():
            for i in range(1, len(target_ids)):
                # Create input: source + target prefix up to position i-1
                english_prefix = target_ids[:i]
                en_xx_token_id = self.conditional_tokenizer.convert_tokens_to_ids(self.tgt_lang)

                full_input = torch.cat([
                    source_ids.squeeze(0),
                    torch.tensor([en_xx_token_id], device=self.device),
                    english_prefix
                ]).unsqueeze(0)

                curr_token_id = target_ids[i].item()

                # Get conditional probability
                outputs = self.conditional_model(input_ids=full_input)
                logits = outputs.logits[0, -1]
                probs = F.softmax(logits, dim=-1)
                entmax_probs = entmax15(logits.cpu(), dim=-1).to(self.device)

                # Standard softmax metrics
                entropy = self.entropy(probs)
                topk_ent, topk_sum = self.topk_entropy(probs, k=5)
                token_prob = probs[curr_token_id].item()
                surprisal = -math.log2(token_prob) if token_prob > 0 else 20.0

                entropies.append(entropy)
                surprisals.append(surprisal)
                topk_entropies.append(topk_ent)
                topk_masses.append(topk_sum)

                # Entmax15 metrics
                entmax_entropy = self.entmax_entropy(entmax_probs)
                entmax_topk_ent, entmax_topk_sum = self.topk_entmax_entropy(entmax_probs, k=5)
                entmax_token_prob = entmax_probs[curr_token_id].item()
                entmax_surprisal = -math.log2(entmax_token_prob) if entmax_token_prob > 0 else 20.0
                entmax_branching = torch.count_nonzero(entmax_probs).item()

                entmax_entropies.append(entmax_entropy)
                entmax_surprisals.append(entmax_surprisal)
                entmax_topk_entropies.append(entmax_topk_ent)
                entmax_topk_masses.append(entmax_topk_sum)
                entmax_branchings.append(entmax_branching)

        # Pad for first token
        for lst in [entropies, surprisals, topk_entropies, topk_masses,
                   entmax_entropies, entmax_surprisals, entmax_topk_entropies, entmax_topk_masses]:
            lst.insert(0, 0.0)
        entmax_branchings.insert(0, 0)

        # Pad to match target length
        while len(entropies) < len(target_ids):
            for lst in [entropies, surprisals, topk_entropies, topk_masses,
                       entmax_entropies, entmax_surprisals, entmax_topk_entropies, entmax_topk_masses]:
                lst.append(0.0)
            entmax_branchings.append(0)

        return {
            "conditional_target_entropy_bits": dict(zip(target_tokens, entropies)),
            "conditional_target_surprisal_bits": dict(zip(target_tokens, surprisals)),
            "conditional_target_topk_entropy_k5_bits": dict(zip(target_tokens, topk_entropies)),
            "conditional_target_topk_mass_k5": dict(zip(target_tokens, topk_masses)),
            "conditional_target_entmax_entropy_bits": dict(zip(target_tokens, entmax_entropies)),
            "conditional_target_entmax_surprisal_bits": dict(zip(target_tokens, entmax_surprisals)),
            "conditional_target_entmax_topk_entropy_k5_bits": dict(zip(target_tokens, entmax_topk_entropies)),
            "conditional_target_entmax_topk_mass_k5": dict(zip(target_tokens, entmax_topk_masses)),
            "conditional_target_entmax_branching_choices": dict(zip(target_tokens, entmax_branchings))
        }

    def _calculate_conditional_target_bigrams(self, source_sentence: str, target_sentence: str) -> Dict[str, Dict[str, float]]:
        """Calculate conditional target bigrams using conditional model."""
        self.conditional_tokenizer.src_lang = self.src_lang
        source_inputs = self.conditional_tokenizer(source_sentence, return_tensors="pt").to(self.device)

        self.conditional_tokenizer.src_lang = self.tgt_lang
        target_inputs = self.conditional_tokenizer(target_sentence, return_tensors="pt").to(self.device)
        target_ids = target_inputs.input_ids
        target_tokens = self.conditional_tokenizer.convert_ids_to_tokens(target_ids[0])

        decoder_input_ids = self.shift_tokens_right(target_ids, self.conditional_tokenizer.pad_token_id)

        entropies, surprisals, topk_entropies, topk_masses = [], [], [], []
        entmax_entropies, entmax_surprisals, entmax_topk_entropies, entmax_topk_masses, entmax_branchings = [], [], [], [], []

        with torch.no_grad():
            outputs = self.conditional_model(input_ids=source_inputs.input_ids, decoder_input_ids=decoder_input_ids)
            logits = outputs.logits
            probs = F.softmax(logits, dim=-1)

            for i in range(1, target_ids.size(1)):
                if i < probs.size(1):
                    prob_dist = probs[0, i]
                    entmax_prob_dist = entmax15(logits[0, i].cpu(), dim=-1).to(self.device)

                    # Standard softmax metrics
                    entropy = self.entropy(prob_dist)
                    topk_ent, topk_sum = self.topk_entropy(prob_dist, k=5)
                    curr_token_id = target_ids[0, i].item()
                    surprisal = -math.log2(prob_dist[curr_token_id].item()) if prob_dist[curr_token_id] > 0 else 20.0

                    entropies.append(entropy)
                    surprisals.append(surprisal)
                    topk_entropies.append(topk_ent)
                    topk_masses.append(topk_sum)

                    # Entmax15 metrics
                    entmax_entropy = self.entmax_entropy(entmax_prob_dist)
                    entmax_topk_ent, entmax_topk_sum = self.topk_entmax_entropy(entmax_prob_dist, k=5)
                    entmax_surprisal = -math.log2(entmax_prob_dist[curr_token_id].item()) if entmax_prob_dist[curr_token_id] > 0 else 20.0
                    entmax_branching = torch.count_nonzero(entmax_prob_dist).item()

                    entmax_entropies.append(entmax_entropy)
                    entmax_surprisals.append(entmax_surprisal)
                    entmax_topk_entropies.append(entmax_topk_ent)
                    entmax_topk_masses.append(entmax_topk_sum)
                    entmax_branchings.append(entmax_branching)
                else:
                    for lst in [entropies, surprisals, topk_entropies, topk_masses,
                               entmax_entropies, entmax_surprisals, entmax_topk_entropies, entmax_topk_masses]:
                        lst.append(0.0)
                    entmax_branchings.append(0)

        # Pad for first token
        for lst in [entropies, surprisals, topk_entropies, topk_masses,
                   entmax_entropies, entmax_surprisals, entmax_topk_entropies, entmax_topk_masses]:
            lst.insert(0, 0.0)
        entmax_branchings.insert(0, 0)

        # Ensure correct length
        while len(entropies) < len(target_tokens):
            for lst in [entropies, surprisals, topk_entropies, topk_masses,
                       entmax_entropies, entmax_surprisals, entmax_topk_entropies, entmax_topk_masses]:
                lst.append(0.0)
            entmax_branchings.append(0)

        return {
            "conditional_bigram_target_entropy_bits": dict(zip(target_tokens, entropies)),
            "conditional_bigram_target_surprisal_bits": dict(zip(target_tokens, surprisals)),
            "conditional_bigram_target_topk_entropy_k5_bits": dict(zip(target_tokens, topk_entropies)),
            "conditional_bigram_target_topk_mass_k5": dict(zip(target_tokens, topk_masses)),
            "conditional_bigram_target_entmax_entropy_bits": dict(zip(target_tokens, entmax_entropies)),
            "conditional_bigram_target_entmax_surprisal_bits": dict(zip(target_tokens, entmax_surprisals)),
            "conditional_bigram_target_entmax_topk_entropy_k5_bits": dict(zip(target_tokens, entmax_topk_entropies)),
            "conditional_bigram_target_entmax_topk_mass_k5": dict(zip(target_tokens, entmax_topk_masses)),
            "conditional_bigram_target_entmax_branching_choices": dict(zip(target_tokens, entmax_branchings))
        }

    # ==================== MANUAL AUTOREGRESSIVE ANALYSIS ====================

    def analyze_manual_autoregressive(self, sentences: List[str], num_beams: int = 10,
                                    num_return_sequences: int = 10) -> Dict[str, Any]:
        """
        MANUAL_AUTOREGRESSIVE Analysis: Manual masking with autoregressive context.
        Uses both seq2seq and conditional models - FIXED VERSION.
        """
        print("=== MANUAL AUTOREGRESSIVE ANALYSIS (FIXED) ===")
        results = {}

        for sentence in sentences:
            print(f"Processing MANUAL AUTOREGRESSIVE analysis for: {sentence}")

            # Source analysis with both models - FIXED
            seq2seq_source_metrics, source_tokens = self._analyze_manual_autoregressive_source_seq2seq_fixed(sentence)
            conditional_source_metrics, _ = self._analyze_manual_autoregressive_source_conditional_fixed(sentence)
            bigram_source_metrics = self._analyze_manual_bigram_decoder_source_fixed(sentence, source_tokens)

            # Get spaCy annotations with FIXED alignment
            doc_src = self.nlp_src(sentence) if self.nlp_src else None
            aligned_src = self.align_spacy_tags(source_tokens, doc_src, sentence)

            source_info = {
                "tokens": source_tokens,
                **seq2seq_source_metrics,
                **conditional_source_metrics,
                **bigram_source_metrics,
                "pos": aligned_src["pos"],
                "dep": aligned_src["dep"],
                "spacy_tokens": aligned_src["spacy_tokens"],
                "analysis_type": "manual_autoregressive_fixed"
            }

            # Target analysis
            candidates = self._analyze_manual_autoregressive_targets_fixed(sentence, num_beams, num_return_sequences)

            results[sentence] = {
                'source_info': source_info,
                'candidates': candidates
            }

        return results

    def _analyze_manual_autoregressive_source_seq2seq_fixed(self, sentence: str) -> Tuple[Dict[str, Dict[str, float]], List[str]]:
        """FIXED: Analyze source via autoregressive generation with proper masking and statistics."""
        self.seq2seq_tokenizer.src_lang = self.src_lang
        inputs = self.seq2seq_tokenizer(sentence, return_tensors="pt").to(self.device)
        input_ids = inputs.input_ids[0]
        source_tokens = self.seq2seq_tokenizer.convert_ids_to_tokens(input_ids)

        print(f"Analyzing {len(source_tokens)} source tokens with seq2seq model")

        entropies, surprisals, topk_entropies, topk_masses = [], [], [], []
        entmax_entropies, entmax_surprisals, entmax_topk_entropies, entmax_topk_masses, entmax_branchings = [], [], [], [], []

        with torch.no_grad():
            for i in range(len(input_ids)):
                if i == 0:
                    # First token - no previous context, assign zeros
                    for lst in [entropies, surprisals, topk_entropies, topk_masses,
                               entmax_entropies, entmax_surprisals, entmax_topk_entropies, entmax_topk_masses]:
                        lst.append(0.0)
                    entmax_branchings.append(0)
                else:
                    # FIXED: Proper autoregressive context handling
                    # Use all previous tokens to predict current token
                    context_ids = input_ids[:i]
                    true_token_id = input_ids[i].item()

                    try:
                        # Create proper encoder-decoder setup for mBART
                        # For autoregressive analysis, we use the context as input
                        encoder_inputs = context_ids.unsqueeze(0)

                        # Create decoder input (for mBART, decoder input is shifted)
                        decoder_input_ids = self.shift_tokens_right(
                            encoder_inputs,
                            self.seq2seq_tokenizer.pad_token_id
                        )

                        # Get model outputs for next token prediction
                        outputs = self.seq2seq_model(
                            input_ids=encoder_inputs,
                            decoder_input_ids=decoder_input_ids
                        )

                        # Get logits for the last position (predicting next token)
                        logits = outputs.logits[0, -1]
                        probs = F.softmax(logits, dim=-1)
                        entmax_probs = entmax15(logits.cpu(), dim=-1).to(self.device)

                        # Standard softmax metrics
                        entropy = self.entropy(probs)
                        topk_ent, topk_sum = self.topk_entropy(probs, k=5)
                        token_prob = probs[true_token_id].item()
                        surprisal = -math.log2(token_prob) if token_prob > 0 else 20.0

                        entropies.append(entropy)
                        surprisals.append(surprisal)
                        topk_entropies.append(topk_ent)
                        topk_masses.append(topk_sum)

                        # Entmax15 metrics
                        entmax_entropy = self.entmax_entropy(entmax_probs)
                        entmax_topk_ent, entmax_topk_sum = self.topk_entmax_entropy(entmax_probs, k=5)
                        entmax_token_prob = entmax_probs[true_token_id].item()
                        entmax_surprisal = -math.log2(entmax_token_prob) if entmax_token_prob > 0 else 20.0
                        entmax_branching = torch.count_nonzero(entmax_probs).item()

                        entmax_entropies.append(entmax_entropy)
                        entmax_surprisals.append(entmax_surprisal)
                        entmax_topk_entropies.append(entmax_topk_ent)
                        entmax_topk_masses.append(entmax_topk_sum)
                        entmax_branchings.append(entmax_branching)

                    except Exception as e:
                        print(f"Error processing token {i} ({source_tokens[i]}): {e}")
                        # Fallback values
                        for lst in [entropies, surprisals, topk_entropies, topk_masses,
                                   entmax_entropies, entmax_surprisals, entmax_topk_entropies, entmax_topk_masses]:
                            lst.append(0.0)
                        entmax_branchings.append(0)

        metrics = {
            "manual_autoregressive_seq2seq_source_entropy_bits": dict(zip(source_tokens, entropies)),
            "manual_autoregressive_seq2seq_source_surprisal_bits": dict(zip(source_tokens, surprisals)),
            "manual_autoregressive_seq2seq_source_topk_entropy_k5_bits": dict(zip(source_tokens, topk_entropies)),
            "manual_autoregressive_seq2seq_source_topk_mass_k5": dict(zip(source_tokens, topk_masses)),
            "manual_autoregressive_seq2seq_source_entmax_entropy_bits": dict(zip(source_tokens, entmax_entropies)),
            "manual_autoregressive_seq2seq_source_entmax_surprisal_bits": dict(zip(source_tokens, entmax_surprisals)),
            "manual_autoregressive_seq2seq_source_entmax_topk_entropy_k5_bits": dict(zip(source_tokens, entmax_topk_entropies)),
            "manual_autoregressive_seq2seq_source_entmax_topk_mass_k5": dict(zip(source_tokens, entmax_topk_masses)),
            "manual_autoregressive_seq2seq_source_entmax_branching_choices": dict(zip(source_tokens, entmax_branchings))
        }

        return metrics, source_tokens

    def _analyze_manual_autoregressive_source_conditional_fixed(self, sentence: str) -> Tuple[Dict[str, Dict[str, float]], List[str]]:
        """FIXED: Analyze source via conditional autoregressive generation."""
        self.conditional_tokenizer.src_lang = self.src_lang
        inputs = self.conditional_tokenizer(sentence, return_tensors="pt").to(self.device)
        input_ids = inputs.input_ids[0]
        source_tokens = self.conditional_tokenizer.convert_ids_to_tokens(input_ids)

        entropies, surprisals, topk_entropies, topk_masses = [], [], [], []
        entmax_entropies, entmax_surprisals, entmax_topk_entropies, entmax_topk_masses, entmax_branchings = [], [], [], [], []

        with torch.no_grad():
            for i in range(len(input_ids)):
                if i == 0:
                    # First token - no previous context
                    for lst in [entropies, surprisals, topk_entropies, topk_masses,
                               entmax_entropies, entmax_surprisals, entmax_topk_entropies, entmax_topk_masses]:
                        lst.append(0.0)
                    entmax_branchings.append(0)
                else:
                    # FIXED: Proper conditional probability calculation
                    context_ids = input_ids[:i]
                    true_token_id = input_ids[i].item()

                    try:
                        # Use conditional model to get next token probabilities
                        context = context_ids.unsqueeze(0)
                        outputs = self.conditional_model(input_ids=context)

                        # Get logits for predicting the next token
                        logits = outputs.logits[0, -1]
                        probs = F.softmax(logits, dim=-1)
                        entmax_probs = entmax15(logits.cpu(), dim=-1).to(self.device)

                        # Standard softmax metrics
                        entropy = self.entropy(probs)
                        topk_ent, topk_sum = self.topk_entropy(probs, k=5)
                        token_prob = probs[true_token_id].item()
                        surprisal = -math.log2(token_prob) if token_prob > 0 else 20.0

                        entropies.append(entropy)
                        surprisals.append(surprisal)
                        topk_entropies.append(topk_ent)
                        topk_masses.append(topk_sum)

                        # Entmax15 metrics
                        entmax_entropy = self.entmax_entropy(entmax_probs)
                        entmax_topk_ent, entmax_topk_sum = self.topk_entmax_entropy(entmax_probs, k=5)
                        entmax_token_prob = entmax_probs[true_token_id].item()
                        entmax_surprisal = -math.log2(entmax_token_prob) if entmax_token_prob > 0 else 20.0
                        entmax_branching = torch.count_nonzero(entmax_probs).item()

                        entmax_entropies.append(entmax_entropy)
                        entmax_surprisals.append(entmax_surprisal)
                        entmax_topk_entropies.append(entmax_topk_ent)
                        entmax_topk_masses.append(entmax_topk_sum)
                        entmax_branchings.append(entmax_branching)

                    except Exception as e:
                        print(f"Error processing conditional token {i}: {e}")
                        # Fallback values
                        for lst in [entropies, surprisals, topk_entropies, topk_masses,
                                   entmax_entropies, entmax_surprisals, entmax_topk_entropies, entmax_topk_masses]:
                            lst.append(0.0)
                        entmax_branchings.append(0)

        metrics = {
            "manual_autoregressive_conditional_source_entropy_bits": dict(zip(source_tokens, entropies)),
            "manual_autoregressive_conditional_source_surprisal_bits": dict(zip(source_tokens, surprisals)),
            "manual_autoregressive_conditional_source_topk_entropy_k5_bits": dict(zip(source_tokens, topk_entropies)),
            "manual_autoregressive_conditional_source_topk_mass_k5": dict(zip(source_tokens, topk_masses)),
            "manual_autoregressive_conditional_source_entmax_entropy_bits": dict(zip(source_tokens, entmax_entropies)),
            "manual_autoregressive_conditional_source_entmax_surprisal_bits": dict(zip(source_tokens, entmax_surprisals)),
            "manual_autoregressive_conditional_source_entmax_topk_entropy_k5_bits": dict(zip(source_tokens, entmax_topk_entropies)),
            "manual_autoregressive_conditional_source_entmax_topk_mass_k5": dict(zip(source_tokens, entmax_topk_masses)),
            "manual_autoregressive_conditional_source_entmax_branching_choices": dict(zip(source_tokens, entmax_branchings))
        }

        return metrics, source_tokens

    def _analyze_manual_bigram_decoder_source_fixed(self, sentence: str, source_tokens: List[str]) -> Dict[str, Dict[str, float]]:
        """FIXED: Analyze source via bigram context with proper decoder masking."""
        self.conditional_tokenizer.src_lang = self.src_lang
        inputs = self.conditional_tokenizer(sentence, return_tensors="pt").to(self.device)
        input_ids = inputs.input_ids[0]

        entropies, surprisals, topk_entropies, topk_masses = [], [], [], []
        entmax_entropies, entmax_surprisals, entmax_topk_entropies, entmax_topk_masses, entmax_branchings = [], [], [], [], []

        with torch.no_grad():
            for i in range(len(input_ids)):
                if i == 0:
                    # First token - no previous context
                    for lst in [entropies, surprisals, topk_entropies, topk_masses,
                               entmax_entropies, entmax_surprisals, entmax_topk_entropies, entmax_topk_masses]:
                        lst.append(0.0)
                    entmax_branchings.append(0)
                else:
                    # FIXED: Proper bigram context analysis
                    prev_token_id = input_ids[i-1:i]  # Previous token only
                    true_token_id = input_ids[i].item()

                    try:
                        # Create bigram context for prediction
                        bigram_context = prev_token_id.unsqueeze(0)
                        outputs = self.conditional_model(input_ids=bigram_context)
                        logits = outputs.logits[0, -1]  # Last position logits

                        probs = F.softmax(logits, dim=-1)
                        entmax_probs = entmax15(logits.cpu(), dim=-1).to(self.device)

                        # Standard softmax metrics
                        entropy = self.entropy(probs)
                        topk_ent, topk_sum = self.topk_entropy(probs, k=5)
                        token_prob = probs[true_token_id].item()
                        surprisal = -math.log2(token_prob) if token_prob > 0 else 20.0

                        entropies.append(entropy)
                        surprisals.append(surprisal)
                        topk_entropies.append(topk_ent)
                        topk_masses.append(topk_sum)

                        # Entmax15 metrics
                        entmax_entropy = self.entmax_entropy(entmax_probs)
                        entmax_topk_ent, entmax_topk_sum = self.topk_entmax_entropy(entmax_probs, k=5)
                        entmax_token_prob = entmax_probs[true_token_id].item()
                        entmax_surprisal = -math.log2(entmax_token_prob) if entmax_token_prob > 0 else 20.0
                        entmax_branching = torch.count_nonzero(entmax_probs).item()

                        entmax_entropies.append(entmax_entropy)
                        entmax_surprisals.append(entmax_surprisal)
                        entmax_topk_entropies.append(entmax_topk_ent)
                        entmax_topk_masses.append(entmax_topk_sum)
                        entmax_branchings.append(entmax_branching)

                    except Exception as e:
                        print(f"Error processing bigram token {i}: {e}")
                        # Fallback values
                        for lst in [entropies, surprisals, topk_entropies, topk_masses,
                                   entmax_entropies, entmax_surprisals, entmax_topk_entropies, entmax_topk_masses]:
                            lst.append(0.0)
                        entmax_branchings.append(0)

        metrics = {
            "manual_bigram_decoder_source_entropy_bits": dict(zip(source_tokens, entropies)),
            "manual_bigram_decoder_source_surprisal_bits": dict(zip(source_tokens, surprisals)),
            "manual_bigram_decoder_source_topk_entropy_k5_bits": dict(zip(source_tokens, topk_entropies)),
            "manual_bigram_decoder_source_topk_mass_k5": dict(zip(source_tokens, topk_masses)),
            "manual_bigram_decoder_source_entmax_entropy_bits": dict(zip(source_tokens, entmax_entropies)),
            "manual_bigram_decoder_source_entmax_surprisal_bits": dict(zip(source_tokens, entmax_surprisals)),
            "manual_bigram_decoder_source_entmax_topk_entropy_k5_bits": dict(zip(source_tokens, entmax_topk_entropies)),
            "manual_bigram_decoder_source_entmax_topk_mass_k5": dict(zip(source_tokens, entmax_topk_masses)),
            "manual_bigram_decoder_source_entmax_branching_choices": dict(zip(source_tokens, entmax_branchings))
        }

        return metrics

    def _analyze_manual_autoregressive_targets_fixed(self, sentence: str, num_beams: int, num_return_sequences: int) -> List[Dict[str, Any]]:
        """FIXED: Analyze targets using manual autoregressive approach."""
        # Generate candidates using seq2seq model
        self.seq2seq_tokenizer.src_lang = self.src_lang
        inputs = self.seq2seq_tokenizer(sentence, return_tensors="pt").to(self.device)

        with torch.no_grad():
            beam_outputs = self.seq2seq_model.generate(
                inputs.input_ids,
                max_new_tokens=50,
                num_beams=num_beams,
                num_return_sequences=num_return_sequences,
                forced_bos_token_id=self.forced_bos_token_id,
                early_stopping=True
            )

        candidates = []
        for seq_idx, sequence in enumerate(beam_outputs):
            candidate = self._process_manual_autoregressive_target_candidate_fixed(sequence, sentence, seq_idx)
            candidates.append(candidate)

        return candidates

    def _process_manual_autoregressive_target_candidate_fixed(self, sequence: torch.Tensor, source_sentence: str, seq_idx: int) -> Dict[str, Any]:
        """FIXED: Process target candidate with proper entropy calculation."""
        decoded = self.seq2seq_tokenizer.decode(sequence, skip_special_tokens=True)
        target_tokens = self.seq2seq_tokenizer.convert_ids_to_tokens(sequence)

        # Get spaCy annotations with FIXED alignment
        doc_tgt = self.nlp_tgt(decoded) if self.nlp_tgt else None
        aligned_tgt = self.align_spacy_tags(target_tokens, doc_tgt, decoded)

        # Calculate comprehensive target metrics with FIXED implementations
        seq2seq_metrics = self._calculate_target_metrics_fixed(sequence, "autoregressive_seq2seq", source_sentence)
        conditional_metrics = self._calculate_target_metrics_fixed(sequence, "autoregressive_conditional", source_sentence)
        bigram_metrics = self._calculate_target_metrics_fixed(sequence, "bigram_decoder", source_sentence)

        return {
            "sentence": decoded,
            "tokens": target_tokens,
            **seq2seq_metrics,
            **conditional_metrics,
            **bigram_metrics,
            "pos": aligned_tgt["pos"],
            "dep": aligned_tgt["dep"],
            "spacy_tokens": aligned_tgt["spacy_tokens"],
            "candidate_idx": seq_idx
        }

    def _calculate_target_metrics_fixed(self, sequence: torch.Tensor, method: str, source_sentence: str = "") -> Dict[str, Dict[str, float]]:
        """FIXED: Calculate target metrics using the specified method with proper probability calculation."""
        target_tokens = self.seq2seq_tokenizer.convert_ids_to_tokens(sequence)

        entropies, surprisals, topk_entropies, topk_masses = [], [], [], []
        entmax_entropies, entmax_surprisals, entmax_topk_entropies, entmax_topk_masses, entmax_branchings = [], [], [], [], []

        with torch.no_grad():
            for i in range(len(sequence)):
                if i == 0:
                    # First token - assign zeros
                    for lst in [entropies, surprisals, topk_entropies, topk_masses,
                               entmax_entropies, entmax_surprisals, entmax_topk_entropies, entmax_topk_masses]:
                        lst.append(0.0)
                    entmax_branchings.append(0)
                    continue 
                # Add this debug block right before the error occurs in _calculate_target_metrics_fixed
                if i == 22:  # Debug token 22
                    print(f"\n=== DEBUG TOKEN 22 ===")
                    print(f"Method: {method}")
                    print(f"Sequence length: {len(sequence)}")
                    print(f"Current position i: {i}")
                    print(f"True token ID: {true_token_id}")
                    print(f"Token text: {self.seq2seq_tokenizer.decode([true_token_id]) if true_token_id < self.seq2seq_tokenizer.vocab_size else 'OUT OF RANGE'}")
    
                    if method == "bigram_decoder" and i >= 1:
                        prev_token = sequence[i-1].item()
                        print(f"Previous token ID: {prev_token}")
                        print(f"Previous token text: {self.seq2seq_tokenizer.decode([prev_token])}")
                        print(f"Bigram context shape: {bigram_context.shape if 'bigram_context' in locals() else 'Not created yet'}")
    
                    if 'logits' in locals():
                        print(f"Logits shape: {logits.shape}")
                        print(f"Vocab size from logits: {logits.size(-1)}")
                        print(f"Token ID within range? {true_token_id < logits.size(-1)}")
    
                    print(f"Seq2seq vocab size: {self.seq2seq_tokenizer.vocab_size}")
                    print(f"Conditional vocab size: {self.conditional_tokenizer.vocab_size}")
                    print(f"Conditional model vocab: {self.conditional_model.config.vocab_size}")
                    # Clear all CUDA cache and reset
                    torch.cuda.empty_cache()
                    gc.collect()

                    # If the error persists, try resetting the GPU
                    if torch.cuda.is_available():
                        torch.cuda.reset_peak_memory_stats()
                        torch.cuda.synchronize()
                    print("=== END DEBUG ===\n")
                else:
                    true_token_id = sequence[i].item()

                    try:
                        if method == "autoregressive_seq2seq":
                            # Use seq2seq model for autoregressive prediction
                            context = sequence[:i].unsqueeze(0)
                            decoder_input = self.shift_tokens_right(context, self.seq2seq_tokenizer.pad_token_id)
                            outputs = self.seq2seq_model(input_ids=context, decoder_input_ids=decoder_input)
                            logits = outputs.logits[0, -1]

                        elif method == "autoregressive_conditional":
                            # Use conditional model for autoregressive prediction
                            context = sequence[:i].unsqueeze(0)
                            outputs = self.conditional_model(input_ids=context)
                            logits = outputs.logits[0, -1]

                        elif method == "bigram_decoder":
                            # Use only previous token for bigram prediction
                            if i >= 1:
                                bigram_context = sequence[i-1:i].unsqueeze(0)
                                outputs = self.conditional_model(input_ids=bigram_context)
                                logits = outputs.logits[0, -1]
                            else:
                                # Fallback for edge case
                                for lst in [entropies, surprisals, topk_entropies, topk_masses,
                                           entmax_entropies, entmax_surprisals, entmax_topk_entropies, entmax_topk_masses]:
                                    lst.append(0.0)
                                entmax_branchings.append(0)
                                continue

                        # Calculate probabilities and metrics
                        probs = F.softmax(logits, dim=-1)
                        entmax_probs = entmax15(logits.cpu(), dim=-1).to(self.device)

                        # Standard softmax metrics
                        entropy = self.entropy(probs)
                        topk_ent, topk_sum = self.topk_entropy(probs, k=5)
                        token_prob = probs[true_token_id].item()
                        surprisal = -math.log2(token_prob) if token_prob > 0 else 20.0

                        entropies.append(entropy)
                        surprisals.append(surprisal)
                        topk_entropies.append(topk_ent)
                        topk_masses.append(topk_sum)

                        # Entmax15 metrics
                        entmax_entropy = self.entmax_entropy(entmax_probs)
                        entmax_topk_ent, entmax_topk_sum = self.topk_entmax_entropy(entmax_probs, k=5)
                        entmax_token_prob = entmax_probs[true_token_id].item()
                        entmax_surprisal = -math.log2(entmax_token_prob) if entmax_token_prob > 0 else 20.0
                        entmax_branching = torch.count_nonzero(entmax_probs).item()

                        entmax_entropies.append(entmax_entropy)
                        entmax_surprisals.append(entmax_surprisal)
                        entmax_topk_entropies.append(entmax_topk_ent)
                        entmax_topk_masses.append(entmax_topk_sum)
                        entmax_branchings.append(entmax_branching)

                    except Exception as e:
                        print(f"Error calculating {method} metrics for token {i}: {e}")
                        # Fallback values
                        for lst in [entropies, surprisals, topk_entropies, topk_masses,
                                   entmax_entropies, entmax_surprisals, entmax_topk_entropies, entmax_topk_masses]:
                            lst.append(0.0)
                        entmax_branchings.append(0)

        # Create method-specific metric keys
        if method == "autoregressive_seq2seq":
            prefix = "manual_autoregressive_seq2seq_target"
        elif method == "autoregressive_conditional":
            prefix = "manual_autoregressive_conditional_target"
        elif method == "bigram_decoder":
            prefix = "manual_bigram_decoder_target"
        else:
            prefix = f"manual_{method}_target"

        return {
            f"{prefix}_entropy_bits": dict(zip(target_tokens, entropies)),
            f"{prefix}_surprisal_bits": dict(zip(target_tokens, surprisals)),
            f"{prefix}_topk_entropy_k5_bits": dict(zip(target_tokens, topk_entropies)),
            f"{prefix}_topk_mass_k5": dict(zip(target_tokens, topk_masses)),
            f"{prefix}_entmax_entropy_bits": dict(zip(target_tokens, entmax_entropies)),
            f"{prefix}_entmax_surprisal_bits": dict(zip(target_tokens, entmax_surprisals)),
            f"{prefix}_entmax_topk_entropy_k5_bits": dict(zip(target_tokens, entmax_topk_entropies)),
            f"{prefix}_entmax_topk_mass_k5": dict(zip(target_tokens, entmax_topk_masses)),
            f"{prefix}_entmax_branching_choices": dict(zip(target_tokens, entmax_branchings))
        }

    # ==================== UNIFIED ANALYSIS METHOD ====================

    def analyze_all_methods(self, sentences: List[str], num_beams: int = 10,
                           num_return_sequences: int = 10) -> Dict[str, Dict[str, Any]]:
        """
        Run all analysis methods and return comprehensive results - FIXED VERSION.

        Args:
            sentences: List of source sentences to analyze
            num_beams: Number of beams for beam search
            num_return_sequences: Number of translation candidates to return

        Returns:
            Dictionary with results from all four analysis methods
        """
        print("=== RUNNING ALL ANALYSIS METHODS (FIXED VERSION) ===")

        all_results = {}

        # 1. FULL Analysis (seq2seq model)
        print("Running FULL analysis...")
        full_results = self.analyze_full(sentences, num_beams, num_return_sequences)
        all_results['full'] = full_results

        # 2. CONDITIONAL Analysis (conditional model)
        print("Running CONDITIONAL analysis...")
        conditional_results = self.analyze_conditional(sentences, num_beams, num_return_sequences)
        all_results['conditional'] = conditional_results

        # 3. MANUAL AUTOREGRESSIVE Analysis (both models) - FIXED
        print("Running MANUAL AUTOREGRESSIVE analysis (FIXED)...")
        manual_auto_results = self.analyze_manual_autoregressive(sentences, num_beams, num_return_sequences)
        all_results['manual_autoregressive'] = manual_auto_results

        print("=== ALL ANALYSIS METHODS COMPLETED (FIXED VERSION) ===")
        return all_results

    # ==================== UTILITY METHODS ====================

    def save_results(self, results: Dict[str, Any], output_path: str):
        """Save analysis results to JSON file."""
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        print(f"Results saved to {output_path}")

    def export_to_dataframe(self, results: Dict[str, Any], analysis_type: str = "manual_autoregressive") -> pd.DataFrame:
        """
        Export results to pandas DataFrame for analysis.

        Args:
            results: Results from analysis methods
            analysis_type: Which analysis type to export

        Returns:
            DataFrame with token-level metrics
        """
        rows = []

        if analysis_type in results:
            for sentence, data in results[analysis_type].items():
                source_info = data['source_info']

                # Source tokens
                for i, token in enumerate(source_info['tokens']):
                    row = {
                        'sentence': sentence,
                        'token': token,
                        'position': i,
                        'token_type': 'source',
                        'analysis_type': analysis_type,
                        'pos': source_info['pos'][i] if i < len(source_info['pos']) else 'UNK',
                        'dep': source_info['dep'][i] if i < len(source_info['dep']) else 'UNK'
                    }

                    # Add all metric keys for this token
                    for metric_name, metric_dict in source_info.items():
                        if isinstance(metric_dict, dict) and token in metric_dict:
                            row[metric_name] = metric_dict[token]

                    rows.append(row)

                # Target tokens
                for candidate in data['candidates']:
                    for i, token in enumerate(candidate['tokens']):
                        row = {
                            'sentence': sentence,
                            'token': token,
                            'position': i,
                            'token_type': 'target',
                            'analysis_type': analysis_type,
                            'candidate_idx': candidate['candidate_idx'],
                            'target_sentence': candidate['sentence'],
                            'pos': candidate['pos'][i] if i < len(candidate['pos']) else 'UNK',
                            'dep': candidate['dep'][i] if i < len(candidate['dep']) else 'UNK'
                        }

                        # Add all metric keys for this token
                        for metric_name, metric_dict in candidate.items():
                            if isinstance(metric_dict, dict) and token in metric_dict:
                                row[metric_name] = metric_dict[token]

                        rows.append(row)

        return pd.DataFrame(rows)

    def compare_original_vs_fixed(self, original_results: Dict, fixed_results: Dict) -> Dict[str, Any]:
        """
        Compare original results with fixed results to show improvements.
        """
        comparison = {
            "alignment_improvements": {},
            "statistics_fixes": {},
            "surprisal_corrections": {},
            "summary": {}
        }

        for sentence in original_results.keys():
            if sentence in fixed_results:
                orig_source = original_results[sentence]['source_info']
                fixed_source = fixed_results[sentence]['source_info']

                # Compare alignment quality
                orig_pos_unk_count = orig_source['pos'].count('UNK')
                fixed_pos_unk_count = fixed_source['pos'].count('UNK')

                comparison["alignment_improvements"][sentence] = {
                    "original_unk_count": orig_pos_unk_count,
                    "fixed_unk_count": fixed_pos_unk_count,
                    "improvement": orig_pos_unk_count - fixed_pos_unk_count,
                    "total_tokens": len(orig_source['tokens']),
                    "improvement_percentage": ((orig_pos_unk_count - fixed_pos_unk_count) / len(orig_source['tokens'])) * 100
                }

                # Compare statistics (looking for NaN/inf values)
                orig_entropies = orig_source.get('manual_autoregressive_seq2seq_source_entropy_bits', {})
                fixed_entropies = fixed_source.get('manual_autoregressive_seq2seq_source_entropy_bits', {})

                orig_nan_count = sum(1 for v in orig_entropies.values() if v != v or v == float('inf'))  # NaN check
                fixed_nan_count = sum(1 for v in fixed_entropies.values() if v != v or v == float('inf'))

                comparison["statistics_fixes"][sentence] = {
                    "original_nan_count": orig_nan_count,
                    "fixed_nan_count": fixed_nan_count,
                    "nan_reduction": orig_nan_count - fixed_nan_count,
                    "total_entropy_values": len(orig_entropies)
                }

                # Compare surprisal values
                orig_surprisals = list(orig_source.get('manual_autoregressive_seq2seq_source_surprisal_bits', {}).values())
                fixed_surprisals = list(fixed_source.get('manual_autoregressive_seq2seq_source_surprisal_bits', {}).values())

                orig_high_surprisal_count = sum(1 for s in orig_surprisals if s >= 20.0)
                fixed_high_surprisal_count = sum(1 for s in fixed_surprisals if s >= 20.0)

                valid_orig_surprisals = [s for s in orig_surprisals if s < 20.0 and s > 0]
                valid_fixed_surprisals = [s for s in fixed_surprisals if s < 20.0 and s > 0]

                comparison["surprisal_corrections"][sentence] = {
                    "original_high_surprisal_count": orig_high_surprisal_count,
                    "fixed_high_surprisal_count": fixed_high_surprisal_count,
                    "high_surprisal_reduction": orig_high_surprisal_count - fixed_high_surprisal_count,
                    "average_original_surprisal": sum(valid_orig_surprisals) / max(1, len(valid_orig_surprisals)),
                    "average_fixed_surprisal": sum(valid_fixed_surprisals) / max(1, len(valid_fixed_surprisals)),
                    "valid_surprisal_count_original": len(valid_orig_surprisals),
                    "valid_surprisal_count_fixed": len(valid_fixed_surprisals)
                }

        # Calculate overall summary
        total_sentences = len(comparison["alignment_improvements"])
        if total_sentences > 0:
            total_alignment_improvement = sum(item["improvement"] for item in comparison["alignment_improvements"].values())
            total_nan_reduction = sum(item["nan_reduction"] for item in comparison["statistics_fixes"].values())
            total_high_surprisal_reduction = sum(item["high_surprisal_reduction"] for item in comparison["surprisal_corrections"].values())

            comparison["summary"] = {
                "total_sentences_compared": total_sentences,
                "total_alignment_tokens_fixed": total_alignment_improvement,
                "total_nan_values_fixed": total_nan_reduction,
                "total_high_surprisal_values_fixed": total_high_surprisal_reduction,
                "average_alignment_improvement_per_sentence": total_alignment_improvement / total_sentences,
                "average_nan_reduction_per_sentence": total_nan_reduction / total_sentences,
                "average_surprisal_improvement_per_sentence": total_high_surprisal_reduction / total_sentences
            }

        return comparison

    def generate_diagnostic_report(self, results: Dict[str, Any], output_path: str = "diagnostic_report.txt"):
        """Generate a comprehensive diagnostic report."""
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write("=== ENHANCED TRANSLATION ENTROPY ANALYSIS - DIAGNOSTIC REPORT ===\n\n")

            for analysis_type, analysis_results in results.items():
                f.write(f"--- {analysis_type.upper()} ANALYSIS ---\n")
                f.write(f"Number of sentences analyzed: {len(analysis_results)}\n\n")

                for sentence, data in analysis_results.items():
                    f.write(f"Sentence: {sentence}\n")
                    source_info = data['source_info']

                    # Alignment quality
                    unk_count = source_info['pos'].count('UNK')
                    total_tokens = len(source_info['tokens'])
                    f.write(f"  Alignment quality: {total_tokens - unk_count}/{total_tokens} tokens aligned ({((total_tokens - unk_count)/total_tokens)*100:.1f}%)\n")

                    # Statistics quality
                    if 'manual_autoregressive_seq2seq_source_entropy_bits' in source_info:
                        entropies = list(source_info['manual_autoregressive_seq2seq_source_entropy_bits'].values())
                        nan_count = sum(1 for v in entropies if v != v or v == float('inf'))
                        f.write(f"  Statistics quality: {len(entropies) - nan_count}/{len(entropies)} valid entropy values\n")

                        surprisals = list(source_info.get('manual_autoregressive_seq2seq_source_surprisal_bits', {}).values())
                        high_surprisal_count = sum(1 for s in surprisals if s >= 20.0)
                        f.write(f"  Surprisal quality: {len(surprisals) - high_surprisal_count}/{len(surprisals)} realistic surprisal values\n")

                    # Candidate quality
                    f.write(f"  Number of translation candidates: {len(data['candidates'])}\n")
                    f.write("\n")

                f.write("\n")

        print(f"Diagnostic report saved to: {output_path}")


# ==================== EXAMPLE USAGE ====================

def main():
    """Example usage of the Enhanced Translation Entropy Analyzer - FIXED VERSION."""

    # Initialize analyzer
    analyzer = EnhancedTranslationEntropyAnalyzer(
        model_name="facebook/mbart-large-50-many-to-many-mmt",
        src_lang="pl_PL",
        tgt_lang="en_XX"
    )

    # Print available analysis methods
    print("Available Analysis Methods:")
    for method, description in analyzer.get_analysis_descriptions().items():
        print(f"\n{method.upper()}:")
        print(description)

    # Example sentences
    sentences = [
        "Na podwórzu bawił się piłką chłopiec. Chłopiec dał piłkę kotu.",
        "Jak się masz?",
        "To jest przykład zdania w języku polskim."
    ]

    # Run all analysis methods with FIXED implementation
    print("\n" + "="*60)
    print("RUNNING ALL ANALYSIS METHODS (FIXED VERSION)")
    print("="*60)

    all_results = analyzer.analyze_all_methods(
        sentences=sentences,
        num_beams=5,
        num_return_sequences=3
    )

    # Save comprehensive results
    analyzer.save_results(all_results, "comprehensive_entropy_analysis_fixed.json")

    # Export each analysis type to separate DataFrames
    for analysis_type in ['full', 'conditional', 'manual_autoregressive']:
        if analysis_type in all_results:
            df = analyzer.export_to_dataframe({analysis_type: all_results[analysis_type]}, analysis_type)
            print(f"\n{analysis_type.upper()} DataFrame shape: {df.shape}")

            # Save to CSV
            df.to_csv(f"{analysis_type}_analysis_fixed.csv", index=False)
            print(f"Saved {analysis_type} analysis to CSV")

    # Generate diagnostic report
    analyzer.generate_diagnostic_report(all_results, "diagnostic_report_fixed.txt")

    # Display sample comparison for first sentence
    first_sentence = sentences[0]
    print(f"\n" + "="*60)
    print(f"SAMPLE ANALYSIS FOR: '{first_sentence}'")
    print("="*60)

    for analysis_type in ['full', 'conditional', 'manual_autoregressive']:
        if analysis_type in all_results and first_sentence in all_results[analysis_type]:
            source_info = all_results[analysis_type][first_sentence]['source_info']
            print(f"\n{analysis_type.upper()} Analysis:")
            print(f"Tokens: {source_info['tokens'][:5]}")  # First 5 tokens

            # Show alignment quality
            unk_count = source_info['pos'].count('UNK')
            total_tokens = len(source_info['tokens'])
            print(f"Alignment quality: {total_tokens - unk_count}/{total_tokens} tokens properly aligned")

            # Show entropy metrics for first few tokens
            for key in source_info.keys():
                if 'entropy_bits' in key and isinstance(source_info[key], dict):
                    print(f"  {key}:")
                    for i, token in enumerate(source_info['tokens'][:3]):
                        if token in source_info[key]:
                            value = source_info[key][token]
                            if value == value and value != float('inf'):  # Check for valid value
                                print(f"    {token}: {value:.3f}")
                            else:
                                print(f"    {token}: INVALID ({value})")

    print(f"\n" + "="*60)
    print("FIXED ANALYSIS COMPLETE")
    print("Key improvements:")
    print("1. ✓ Fixed spaCy alignment using proper ▁ boundary detection")
    print("2. ✓ Fixed autoregressive statistics with proper encoder-decoder setup")
    print("3. ✓ Fixed surprisal calculations with correct probability extraction")
    print("4. ✓ Added comprehensive error handling and fallback values")
    print("5. ✓ Enhanced diagnostic reporting and comparison capabilities")
    print("="*60)


if __name__ == "__main__":
    main()